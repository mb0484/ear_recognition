{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ear_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpnBNX8i5cSa/jbnquQVg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mb0484/ear_recognition/blob/main/Ear_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLNhZuePK8bg",
        "outputId": "66c53c6e-dd78-456f-ebfc-f185eea608d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.19.2)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2021.5.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.19.5)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.1.2.30)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.0.10)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Ear-classification-7 to folder: 100% [7834304 / 7834304] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Ear-classification-7 in folder:: 100%|██████████| 2753/2753 [00:02<00:00, 1346.12it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"nJmICDv4KQLTtXd7L86q\")\n",
        "project = rf.workspace().project(\"ear-classification\")\n",
        "dataset = project.version(7).download(\"folder\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"nJmICDv4KQLTtXd7L86q\")\n",
        "project = rf.workspace().project(\"ear-classification\")\n",
        "dataset = project.version(5).download(\"folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C1siK0J7-zCa",
        "outputId": "a948abe6-8d2f-47d2-d31b-b823f03074ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-0.2.1.tar.gz (15 kB)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Collecting kiwisolver==1.3.1\n",
            "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 75.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.19.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.19.2-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.62.3)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.0.10)\n",
            "Building wheels for collected packages: roboflow, wget\n",
            "  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for roboflow: filename=roboflow-0.2.1-py3-none-any.whl size=20916 sha256=d323d27e758d3d697f2e0058d28fc174bd17c525d5d58fb36bfca6803e9f1848\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/00/03/2d83f3e90ff41c36eef9c3747c328290c01b06e2619f9ed7b6\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=e594802cac915fdd882a6c176933458390af546fb0ac50b912086a5c2c18434a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built roboflow wget\n",
            "Installing collected packages: urllib3, pyparsing, kiwisolver, cycler, certifi, wget, requests, PyYAML, python-dotenv, chardet, roboflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.6\n",
            "    Uninstalling pyparsing-3.0.6:\n",
            "      Successfully uninstalled pyparsing-3.0.6\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.3.2\n",
            "    Uninstalling kiwisolver-1.3.2:\n",
            "      Successfully uninstalled kiwisolver-1.3.2\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 certifi-2021.5.30 chardet-4.0.0 cycler-0.10.0 kiwisolver-1.3.1 pyparsing-2.4.7 python-dotenv-0.19.2 requests-2.27.1 roboflow-0.2.1 urllib3-1.26.6 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "kiwisolver",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Ear-classification-5 to folder: 100% [7355577 / 7355577] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Ear-classification-5 in folder:: 100%|██████████| 2713/2713 [00:01<00:00, 2089.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Ear-classification-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHQCR-oWLAvs",
        "outputId": "018679a3-1f33-40c4-a027-eac9bac809ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Ear-classification-5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "#from Model import CNN\n",
        "#from Dataset import CatsAndDogsDataset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import tarfile\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets.utils import download_url\n",
        "\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "jDYzJYtXLCkW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(columns=[\"img_name\",\"label\"])\n",
        "\n",
        "train_array = []\n",
        "train_label_array = []\n",
        "\n",
        "for dir in os.listdir(\"train/\"):\n",
        "  cur_class = dir\n",
        "  for img_name in os.listdir(\"train/\" + dir + \"/\"):\n",
        "    train_array.append(img_name)\n",
        "    train_label_array.append(dir)\n",
        "\n",
        "train_df[\"img_name\"] = train_array\n",
        "train_df[\"label\"] = train_label_array\n",
        "\n",
        "train_df.to_csv (r'train_csv.csv', index = False, header=True)\n",
        "\n",
        "test_df = pd.DataFrame(columns=[\"img_name\",\"label\"])\n",
        "\n",
        "test_array = []\n",
        "test_label_array = []\n",
        "\n",
        "for dir in os.listdir(\"test/\"):\n",
        "  cur_class = dir\n",
        "  for img_name in os.listdir(\"test/\" + dir + \"/\"):\n",
        "    test_array.append(img_name)\n",
        "    test_label_array.append(dir)\n",
        "\n",
        "test_df[\"img_name\"] = test_array\n",
        "test_df[\"label\"] = test_label_array\n",
        "\n",
        "test_df.to_csv (r'test_csv.csv', index = False, header=True)\n",
        "\n",
        "valid_df = pd.DataFrame(columns=[\"img_name\",\"label\"])\n",
        "\n",
        "valid_array = []\n",
        "valid_label_array = []\n",
        "\n",
        "for dir in os.listdir(\"valid/\"):\n",
        "  cur_class = dir\n",
        "  for img_name in os.listdir(\"valid/\" + dir + \"/\"):\n",
        "    valid_array.append(img_name)\n",
        "    valid_label_array.append(dir)\n",
        "\n",
        "valid_df[\"img_name\"] = valid_array\n",
        "valid_df[\"label\"] = valid_label_array\n",
        "\n",
        "valid_df.to_csv (r'valid_csv.csv', index = False, header=True)"
      ],
      "metadata": {
        "id": "uO8_hpeSLC8j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class CustomEarsDataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.annotations = pd.read_csv(annotation_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.annotations.iloc[index, 0]\n",
        "        image_label = self.annotations.iloc[index, 1]\n",
        "        new_root_path = self.root_dir + \"/\" + str(image_label)\n",
        "        img = Image.open(os.path.join(new_root_path, img_id)).convert(\"RGB\")\n",
        "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return (img, y_label)"
      ],
      "metadata": {
        "id": "WTXeW4K6LC-6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "        [\n",
        "            #transforms.Resize((356, 356)),\n",
        "            #transforms.RandomCrop((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "jAd_m-jmNCQt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "shuffle = True\n",
        "pin_memory = True\n",
        "num_workers = 2"
      ],
      "metadata": {
        "id": "-6grJLmILDBP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = CustomEarsDataset(\"train\",\"train_csv.csv\",transform=transform)\n",
        "validation_set = CustomEarsDataset(\"valid\",\"valid_csv.csv\",transform=transform)\n",
        "test_set = CustomEarsDataset(\"test\",\"test_csv.csv\",transform=transform)\n",
        "train_loader = DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\n",
        "validation_loader = DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)\n",
        "test_loader = DataLoader(dataset=test_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)"
      ],
      "metadata": {
        "id": "G8VQQoBeLDD3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "metadata": {
        "id": "JNDlXxwMLhI4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, Flatten\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "class Net(Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            #BatchNorm2d(64),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer\n",
        "            #Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            #BatchNorm2d(32),\n",
        "            #ReLU(inplace=True),\n",
        "            #MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer\n",
        "            Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(128),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            #Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(115200, 1024), #4 * 7 * 7\n",
        "            ReLU(),\n",
        "            Dropout(p=0.6, inplace=False),\n",
        "            Linear(1024, 101),\n",
        "            #Softmax(dim = 1)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "class NetEars(Module):   \n",
        "    def __init__(self):\n",
        "        super(NetEars, self).__init__()\n",
        "\n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(64),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(256),\n",
        "            ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(2560, 101), #4 * 7 * 7\n",
        "            #ReLU(),\n",
        "            #Dropout(p=0.6, inplace=False),\n",
        "            #Linear(1024, 101),\n",
        "            #Softmax(dim = 1)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "class ImprovedNet(Module):   \n",
        "    def __init__(self):\n",
        "        super(ImprovedNet, self).__init__()\n",
        "\n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv2d(3, 32, kernel_size=9, stride=1, padding=1),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            BatchNorm2d(32),\n",
        "            Dropout(0.5),\n",
        "\n",
        "            Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            BatchNorm2d(64),\n",
        "            Dropout(0.5),\n",
        "\n",
        "            Conv2d(64, 96, kernel_size=(3,3)),\n",
        "            ReLU(),\n",
        "            BatchNorm2d(96),\n",
        "            Dropout(0.4),\n",
        "            #Flatten(),\n",
        "            #Linear(62976, 100),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Flatten(),\n",
        "            Linear(62976, 101)\n",
        "            #Linear(62976, 101), #4 * 7 * 7\n",
        "            #ReLU(),\n",
        "            #Dropout(p=0.6, inplace=False),\n",
        "            #Linear(1024, 101),\n",
        "            #Softmax(dim = 1)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3C3QOj3nLkxq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank1(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "metadata": {
        "id": "NEKQa9p1LpoU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def test_step(self, batch, function=rank1):\n",
        "        images, labels = batch\n",
        "        out = self(images)                    # Generate predictions\n",
        "        acc = function(out, labels)           # Calculate accuracy\n",
        "        return {'test_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        batch_accs = [x['test_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'test_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "metadata": {
        "id": "ZAhLld2ZLsbv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = myNet\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "metadata": {
        "id": "hcYlcRSMLu4_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')"
      ],
      "metadata": {
        "id": "u3QCX0htLyTz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdJLdXyvLz5o",
        "outputId": "b2eaa49a-54e1-479e-a28d-9fbbe71c2d43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(data, device):\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)"
      ],
      "metadata": {
        "id": "L4NQepunL1_e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "tzgSqLerMB_9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "val_dl = DeviceDataLoader(validation_loader, device)\n",
        "test_dl = DeviceDataLoader(test_loader, device)"
      ],
      "metadata": {
        "id": "uip8c-lqMD-y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()   #eval() is called to tell model that now it is validation mode and so don't perform stuff like dropout,backpropagation etc..\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def test(model, test_loader, function=rank1):\n",
        "    model.eval()   #eval() is called to tell model that now it is validation mode and so don't perform stuff like dropout,backpropagation etc..\n",
        "    outputs = [model.test_step(batch, function) for batch in test_loader]\n",
        "    return model.test_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train() #eval() is called to tell model that now it is training mode and so  perform stuff like dropout,backpropagation etc..\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        print(\"validation\")\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "metadata": {
        "id": "4NyDhoV1MJuf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myNet = Net()\n",
        "my_model = MyModel()\n",
        "to_device(my_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZzbgMNKPcvQ",
        "outputId": "44b9ede3-7f23-43e1-dd24-57fff6c1258b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (network): Net(\n",
              "    (cnn_layers): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (linear_layers): Sequential(\n",
              "      (0): Linear(in_features=115200, out_features=1024, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.6, inplace=False)\n",
              "      (3): Linear(in_features=1024, out_features=101, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(my_model, val_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1ha-ZPqMNjT",
        "outputId": "35a8b511-2917-4e2f-d76a-9a7f0f1f2019"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.010416666977107525, 'val_loss': 4.616459846496582}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 55\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.00001\n",
        "history = fit(num_epochs, lr, my_model, train_dl, val_dl, opt_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twn86pvxNS8s",
        "outputId": "a3c9ff5f-9763-412f-eea1-3b6b6d1c5719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation\n",
            "Epoch [0], train_loss: 4.5134, val_loss: 4.5361, val_acc: 0.0312\n",
            "validation\n",
            "Epoch [1], train_loss: 3.9663, val_loss: 4.3710, val_acc: 0.0417\n",
            "validation\n",
            "Epoch [2], train_loss: 3.4535, val_loss: 4.1119, val_acc: 0.1127\n",
            "validation\n",
            "Epoch [3], train_loss: 2.9276, val_loss: 4.1184, val_acc: 0.1544\n",
            "validation\n",
            "Epoch [4], train_loss: 2.3959, val_loss: 3.8492, val_acc: 0.2140\n",
            "validation\n",
            "Epoch [5], train_loss: 1.8697, val_loss: 3.8865, val_acc: 0.1553\n",
            "validation\n",
            "Epoch [6], train_loss: 1.4652, val_loss: 3.7879, val_acc: 0.1553\n",
            "validation\n",
            "Epoch [7], train_loss: 1.1436, val_loss: 3.7321, val_acc: 0.2159\n",
            "validation\n",
            "Epoch [8], train_loss: 0.9082, val_loss: 3.4650, val_acc: 0.2775\n",
            "validation\n",
            "Epoch [9], train_loss: 0.7262, val_loss: 3.6681, val_acc: 0.2377\n",
            "validation\n",
            "Epoch [10], train_loss: 0.5579, val_loss: 3.4713, val_acc: 0.2481\n",
            "validation\n",
            "Epoch [11], train_loss: 0.4378, val_loss: 3.4406, val_acc: 0.2386\n",
            "validation\n",
            "Epoch [12], train_loss: 0.3564, val_loss: 3.5600, val_acc: 0.2585\n",
            "validation\n",
            "Epoch [13], train_loss: 0.2941, val_loss: 3.6639, val_acc: 0.2888\n",
            "validation\n",
            "Epoch [14], train_loss: 0.2365, val_loss: 3.5048, val_acc: 0.2689\n",
            "validation\n",
            "Epoch [15], train_loss: 0.1948, val_loss: 3.5177, val_acc: 0.2888\n",
            "validation\n",
            "Epoch [16], train_loss: 0.1613, val_loss: 3.3244, val_acc: 0.3286\n",
            "validation\n",
            "Epoch [17], train_loss: 0.1519, val_loss: 3.4225, val_acc: 0.2585\n",
            "validation\n",
            "Epoch [18], train_loss: 0.1239, val_loss: 3.3434, val_acc: 0.2784\n",
            "validation\n",
            "Epoch [19], train_loss: 0.1105, val_loss: 3.2428, val_acc: 0.2680\n",
            "validation\n",
            "Epoch [20], train_loss: 0.0995, val_loss: 3.4627, val_acc: 0.2699\n",
            "validation\n",
            "Epoch [21], train_loss: 0.0853, val_loss: 3.4880, val_acc: 0.2699\n",
            "validation\n",
            "Epoch [22], train_loss: 0.0761, val_loss: 3.2470, val_acc: 0.3097\n",
            "validation\n",
            "Epoch [23], train_loss: 0.0618, val_loss: 3.0777, val_acc: 0.3892\n",
            "validation\n",
            "Epoch [24], train_loss: 0.0568, val_loss: 3.1686, val_acc: 0.3201\n",
            "validation\n",
            "Epoch [25], train_loss: 0.0555, val_loss: 3.2797, val_acc: 0.3201\n",
            "validation\n",
            "Epoch [26], train_loss: 0.0496, val_loss: 3.4079, val_acc: 0.2992\n",
            "validation\n",
            "Epoch [27], train_loss: 0.0457, val_loss: 3.2376, val_acc: 0.3295\n",
            "validation\n",
            "Epoch [28], train_loss: 0.0442, val_loss: 3.2430, val_acc: 0.3097\n",
            "validation\n",
            "Epoch [29], train_loss: 0.0365, val_loss: 3.3710, val_acc: 0.3106\n",
            "validation\n",
            "Epoch [30], train_loss: 0.0328, val_loss: 3.1358, val_acc: 0.2888\n",
            "validation\n",
            "Epoch [31], train_loss: 0.0334, val_loss: 3.1934, val_acc: 0.3201\n",
            "validation\n",
            "Epoch [32], train_loss: 0.0294, val_loss: 3.2983, val_acc: 0.2898\n",
            "validation\n",
            "Epoch [33], train_loss: 0.0274, val_loss: 3.0670, val_acc: 0.3400\n",
            "validation\n",
            "Epoch [34], train_loss: 0.0232, val_loss: 3.3956, val_acc: 0.3504\n",
            "validation\n",
            "Epoch [35], train_loss: 0.0249, val_loss: 3.3693, val_acc: 0.3097\n",
            "validation\n",
            "Epoch [36], train_loss: 0.0206, val_loss: 3.3397, val_acc: 0.3409\n",
            "validation\n",
            "Epoch [37], train_loss: 0.0202, val_loss: 3.4142, val_acc: 0.3201\n",
            "validation\n",
            "Epoch [38], train_loss: 0.0211, val_loss: 3.2284, val_acc: 0.3504\n",
            "validation\n",
            "Epoch [39], train_loss: 0.0179, val_loss: 3.3995, val_acc: 0.3106\n",
            "validation\n",
            "Epoch [40], train_loss: 0.0177, val_loss: 3.1212, val_acc: 0.3494\n",
            "validation\n",
            "Epoch [41], train_loss: 0.0174, val_loss: 3.3165, val_acc: 0.3513\n",
            "validation\n",
            "Epoch [42], train_loss: 0.0151, val_loss: 3.1299, val_acc: 0.3598\n",
            "validation\n",
            "Epoch [43], train_loss: 0.0151, val_loss: 3.3788, val_acc: 0.3106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rank5(outputs, labels):\n",
        "    _, first_predictions = torch.kthvalue(outputs, k=len(outputs[0]) - 0, dim=1)\n",
        "    _, second_predictions = torch.kthvalue(outputs, k=len(outputs[0]) - 1, dim=1)\n",
        "    _, third_predictions = torch.kthvalue(outputs, k=len(outputs[0]) - 2, dim=1)\n",
        "    _, fourth_predictions = torch.kthvalue(outputs, k=len(outputs[0]) - 3, dim=1)\n",
        "    _, fifth_predictions = torch.kthvalue(outputs, k=len(outputs[0]) - 4, dim=1)\n",
        "\n",
        "    sum_right_k_5 = 0;\n",
        "    sum_right_k_5 += torch.sum(first_predictions == labels).item();\n",
        "    sum_right_k_5 += torch.sum(second_predictions == labels).item();\n",
        "    sum_right_k_5 += torch.sum(third_predictions == labels).item();\n",
        "    sum_right_k_5 += torch.sum(fourth_predictions == labels).item();\n",
        "    sum_right_k_5 += torch.sum(fifth_predictions == labels).item();\n",
        "\n",
        "    return torch.tensor(sum_right_k_5 / len(labels))"
      ],
      "metadata": {
        "id": "X9DAdOrQ61bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_device(my_model, device)\n",
        "test(my_model, test_dl, function=rank1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHZUJvy3R27B",
        "outputId": "442f1d82-9d31-49d3-8666-e0e2fec598c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.return_types.kthvalue(\n",
            "values=tensor([11,  7, 30,  7,  6,  1,  7,  7, 38, 25,  7, 20,  7,  9,  6,  2],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([1, 2, 2, 0, 1, 1, 0, 4, 4, 4, 3, 1, 0, 0, 1, 4], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([20,  1,  2, 28, 23, 38, 26,  5, 33,  9, 30, 57, 45,  1, 11, 62],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([2, 3, 2, 1, 1, 4, 1, 4, 3, 1, 4, 2, 0, 0, 3, 2], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([ 8, 15, 24,  7, 23,  7, 25, 16, 52,  7,  3, 24, 12,  2, 13, 60],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([3, 0, 3, 1, 1, 0, 1, 1, 0, 1, 3, 4, 2, 2, 0, 3], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([ 8,  9,  7, 18, 41, 24, 26,  7,  7,  1,  1,  5,  1, 14,  7, 30],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([4, 4, 2, 2, 1, 1, 1, 4, 1, 1, 0, 0, 4, 0, 3, 2], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([10, 20, 11,  1, 20,  1,  1,  2,  7,  1, 52, 25, 13,  7, 28, 10],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([2, 4, 2, 0, 3, 3, 2, 1, 2, 4, 2, 4, 3, 0, 0, 3], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([23,  4, 13,  2,  9, 10, 13, 13,  1, 15,  6, 12, 11, 28,  2, 21],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([3, 1, 0, 4, 1, 0, 2, 4, 0, 4, 4, 4, 3, 4, 1, 4], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([ 1, 13, 13,  1, 10, 24, 16, 29, 11, 26, 20, 46,  1, 28, 38,  7],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([3, 3, 0, 0, 3, 4, 1, 4, 1, 3, 2, 0, 0, 2, 3, 4], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([42,  1,  1, 13,  9, 34,  7,  2, 28,  2,  7, 27,  1, 24, 57, 32],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([4, 3, 2, 4, 1, 4, 0, 2, 0, 0, 0, 4, 2, 2, 1, 3], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([13, 20, 11, 60,  1,  8,  1,  3, 10, 48, 11, 10,  1, 20, 11, 27],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([1, 0, 3, 2, 0, 4, 3, 2, 3, 0, 0, 4, 3, 1, 1, 0], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([ 1, 23, 12, 19, 25, 43, 22,  7, 35,  1, 25, 30,  1,  7,  7,  1],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([3, 3, 4, 2, 1, 3, 3, 0, 1, 3, 3, 3, 3, 1, 3, 0], device='cuda:0'))\n",
            "torch.return_types.kthvalue(\n",
            "values=tensor([10,  1, 24, 61,  1, 35, 10, 12, 14,  1, 22, 13,  8,  9, 48],\n",
            "       device='cuda:0'),\n",
            "indices=tensor([1, 3, 0, 4, 3, 4, 1, 2, 4, 4, 3, 0, 1, 0, 1], device='cuda:0'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_acc': 0.09166666865348816}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(my_model.state_dict(), 'model_trained_.pth')"
      ],
      "metadata": {
        "id": "LnJ2esyIPkBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = MyModel()\n",
        "new_model.load_state_dict(torch.load('model_trained_32.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXXGZIcTyeCw",
        "outputId": "2741ef7a-2b3f-46c4-b4a3-02b4d21d6703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    }
  ]
}